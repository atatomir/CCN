{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle as RectPatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Rectangle:\n",
    "    def __init__(self, x0, x1, y0, y1):\n",
    "        self.lower = np.array([x0, y0])\n",
    "        self.higher = np.array([x1, y1])\n",
    "        \n",
    "    def inside(self, x, y):\n",
    "        point = np.array([x, y])\n",
    "        return ((self.lower <= point) * (point <= self.higher)).sum().item() == 2\n",
    "    \n",
    "    def plot(self, ax, edgecolor = 'blue'):\n",
    "        dim = self.higher - self.lower\n",
    "        ax.add_patch(RectPatch((self.lower[0], self.lower[1]), dim[0], dim[1], fill = False, edgecolor = edgecolor))\n",
    "    \n",
    "rect0 = Rectangle(0.1, 0.6, 0.1, 0.6)\n",
    "rect1 = Rectangle(0.4, 0.9, 0.4, 0.9)\n",
    "\n",
    "def draw_rectangles(ax, rect0, rect1):\n",
    "    plt.axis([0., 1., 0., 1.])\n",
    "    rect0.plot(ax, 'red')\n",
    "    rect1.plot(ax)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "draw_rectangles(ax, rect0, rect1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RectanglesDataset(Dataset):\n",
    "    def __init__(self, count, r0, r1):\n",
    "        super(RectanglesDataset, self).__init__()\n",
    "        self.r0 = r0\n",
    "        self.r1 = r1\n",
    "\n",
    "        self.values = [torch.rand((2,)) for i in range(count)]\n",
    "        self.labels = [self.correct(x, y) for (x, y) in self.values]\n",
    "\n",
    "    def correct(self, x, y):\n",
    "        small = self.r0.inside(x, y)\n",
    "        big = small or self.r1.inside(x, y)\n",
    "        return torch.tensor([1. if v else 0. for v in [small, big]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.values)  \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.values[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = 10000\n",
    "train_data = RectanglesDataset(points, rect0, rect1)\n",
    "test_data = RectanglesDataset(points // 10, rect0, rect1)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size = 64)\n",
    "test_dataloader = DataLoader(test_data, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (linear): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=4, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=4, out_features=2, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(2, 4),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(4, 2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "base = NeuralNetwork()\n",
    "print(base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The CCN framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "n13\n",
      "n12\n",
      "12\n",
      "17\n",
      "n17\n",
      "n19\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "class Literal:\n",
    "    def __init__(self, *args):\n",
    "        if len(args) == 2:\n",
    "            # Literal(int, bool)\n",
    "            self.atom = args[0]\n",
    "            self.positive = args[1]\n",
    "        else:\n",
    "            # Literal(string)\n",
    "            plain = args[0]\n",
    "            if 'n' in plain:\n",
    "                self.atom = int(plain[1:])\n",
    "                self.positive = False\n",
    "            else:\n",
    "                self.atom = int(plain)\n",
    "                self.positive = True\n",
    "            \n",
    "    def neg(self):\n",
    "        return Literal(self.atom, not self.positive)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.atom) if self.positive else 'n' + str(self.atom)\n",
    "    \n",
    "for lit in [Literal('13'), Literal('n12'), Literal(17, True), Literal(19, False)]:\n",
    "    print(str(lit))\n",
    "    print(str(lit.neg()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 :- 0\n",
      "0 :- 1 n2\n",
      "n0 :- n1 2 3\n"
     ]
    }
   ],
   "source": [
    "class Constraint:\n",
    "    def __init__(self, *args):\n",
    "        if len(args) == 2:\n",
    "            # Constraint(Literal, [Literal])\n",
    "            self.head = args[0]\n",
    "            self.body = args[1]\n",
    "        else:\n",
    "            # Constraint(string)\n",
    "            line = args[0].split(' ')\n",
    "            if line[2] == ':-':\n",
    "                line = line[1:]\n",
    "            assert line[1] == ':-'\n",
    "            self.head = Literal(line[0])\n",
    "            self.body = [Literal(lit) for lit in line[2:]]\n",
    "            \n",
    "    def head_encoded(self, num_classes):\n",
    "        pos_head = np.zeros(num_classes)\n",
    "        neg_head = np.zeros(num_classes)\n",
    "        if self.head.positive:\n",
    "            pos_head[self.head.atom] = 1\n",
    "        else:\n",
    "            neg_head[self.head.atom] = 1\n",
    "        return pos_head, neg_head\n",
    "    \n",
    "    def body_encoded(self, num_classes):\n",
    "        pos_body = np.zeros(num_classes, dtype=int)\n",
    "        neg_body = np.zeros(num_classes, dtype=int)\n",
    "        for lit in self.body:\n",
    "            if lit.positive:\n",
    "                pos_body[lit.atom] = 1\n",
    "            else:\n",
    "                neg_body[lit.atom] = 1\n",
    "        return pos_body, neg_body\n",
    "    \n",
    "    def where(self, cond, opt1, opt2):\n",
    "        return opt2 + cond * (opt1 - opt2)\n",
    "    \n",
    "    def coherent_with(self, preds):\n",
    "        num_classes = preds.shape[1]\n",
    "        pos_body, neg_body = self.body_encoded(num_classes)\n",
    "        pos_body = preds[:, pos_body.astype(bool)]\n",
    "        neg_body = 1 - preds[:, neg_body.astype(bool)]\n",
    "        body = np.min(np.concatenate((pos_body, neg_body), axis=1), axis=1)\n",
    "        \n",
    "        head = preds[:, self.head.atom]\n",
    "        if not self.head.positive:\n",
    "            head = 1 - head\n",
    "            \n",
    "        return body <= head\n",
    "        \n",
    "    def __str__(self):\n",
    "        return str(self.head) + \" :- \" + ' '.join([str(lit) for lit in self.body])\n",
    "    \n",
    "    \n",
    "cons0 = Constraint(Literal('1'), [Literal('0')])\n",
    "cons1 = Constraint('0 :- 1 n2')\n",
    "cons2 = Constraint('n0 :- n1 2 3')\n",
    "    \n",
    "for cons in [cons0, cons1, cons2]:\n",
    "    print(cons)\n",
    "\n",
    "assert (cons0.coherent_with(np.array([\n",
    "    [0.1, 0.2],\n",
    "    [0.2, 0.1],\n",
    "    [0.1, 0.1]\n",
    "])) == [True, False, True]).all()\n",
    "\n",
    "assert (cons2.coherent_with(np.array([\n",
    "    [0.7, 0.8, 0.3, 0.4],\n",
    "    [0.8, 0.8, 0.3, 0.4],\n",
    "    [0.9, 0.8, 0.3, 0.4],\n",
    "])) == [True, True, False]).all()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 :- 0\n",
      "0 :- 1 n2\n",
      "n0 :- n1 2 3\n"
     ]
    }
   ],
   "source": [
    "class ConstraintsGroup:\n",
    "    def __init__(self, arg):\n",
    "        if isinstance(arg, list):\n",
    "            # ConstraintGroup([Constraint])\n",
    "            self.constraints = arg\n",
    "        else:\n",
    "            # ConstraintGroup(string)\n",
    "            with open(arg, 'r') as f:\n",
    "                self.constraints = [Constraint(line) for line in f]\n",
    "                \n",
    "    def strata(self):\n",
    "        # TODO: Implement stratification\n",
    "        return [self, self]\n",
    "                \n",
    "    def head_encoded(self, num_classes):\n",
    "        pos_head = []\n",
    "        neg_head = []\n",
    "        \n",
    "        for constraint in self.constraints:\n",
    "            pos, neg = constraint.head_encoded(num_classes)\n",
    "            pos_head.append(pos)\n",
    "            neg_head.append(neg)\n",
    "            \n",
    "        return np.array(pos_head), np.array(neg_head)\n",
    "    \n",
    "    def body_encoded(self, num_classes):\n",
    "        pos_body = []\n",
    "        neg_body = []\n",
    "        \n",
    "        for constraint in self.constraints:\n",
    "            pos, neg = constraint.body_encoded(num_classes)\n",
    "            pos_body.append(pos)\n",
    "            neg_body.append(neg)\n",
    "            \n",
    "        return np.array(pos_body), np.array(neg_body)\n",
    "            \n",
    "    def encoded(self, num_classes):\n",
    "        head = self.head_encoded(num_classes)\n",
    "        body = self.body_encoded(num_classes)\n",
    "        return head, body\n",
    "    \n",
    "    def coherent_with(self, preds):\n",
    "        coherent = [constraint.coherent_with(preds) for constraint in self.constraints]\n",
    "        return np.array(coherent).transpose()\n",
    "            \n",
    "    def __str__(self):\n",
    "        return '\\n'.join([str(constraint) for constraint in self.constraints])\n",
    "        \n",
    "constraints_group = ConstraintsGroup([cons0, cons1, cons2])\n",
    "print(constraints_group)\n",
    "#print(ConstraintGroup('/Users/home/Desktop/PyToys/constraint'))\n",
    "\n",
    "assert (constraints_group.coherent_with(np.array([\n",
    "    [0.1, 0.2, 0.3, 0.4],\n",
    "    [0.7, 0.2, 0.3, 0.4],\n",
    "    [0.8, 0.2, 0.3, 0.4]\n",
    "])) == np.array([\n",
    "     [ True, False,  True],\n",
    "     [False,  True,  True],\n",
    "     [False,  True, False]])).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConstraintsModule()\n"
     ]
    }
   ],
   "source": [
    "class ConstraintsModule(nn.Module):\n",
    "    def __init__(self, constraints_group, num_classes):\n",
    "        super(ConstraintsModule, self).__init__()\n",
    "        head, body = constraints_group.encoded(num_classes)\n",
    "        pos_head, neg_head = head\n",
    "        pos_body, neg_body = body\n",
    "        \n",
    "        self.pos_head = torch.from_numpy(pos_head)\n",
    "        self.neg_head = torch.from_numpy(neg_head)\n",
    "        self.pos_body = torch.from_numpy(pos_body)\n",
    "        self.neg_body = torch.from_numpy(neg_body)\n",
    "        \n",
    "    def where(self, cond, opt1, opt2):\n",
    "        return opt2 + cond * (opt1 - opt2)\n",
    "        \n",
    "    def forward(self, preds):\n",
    "        batch, num = preds.shape[0], preds.shape[1]\n",
    "        cons = self.pos_head.shape[0]\n",
    "        \n",
    "        # batch x cons x num\n",
    "        exp_preds = preds.unsqueeze(1).expand(batch, cons, num)\n",
    "        pos_body = self.pos_body.unsqueeze(0).expand(batch, cons, num)\n",
    "        neg_body = self.neg_body.unsqueeze(0).expand(batch, cons, num)\n",
    "        \n",
    "        # batch x cons\n",
    "        pos_body_min = torch.min(self.where(pos_body, exp_preds, 1), dim=2).values\n",
    "        neg_body_min = torch.min(self.where(neg_body, 1. - exp_preds, 1), dim=2).values\n",
    "        body_min = torch.minimum(pos_body_min, neg_body_min)\n",
    "        \n",
    "        # batch x cons x num\n",
    "        body_min = body_min.unsqueeze(2).expand(batch, cons, num)\n",
    "        pos_head = self.pos_head.unsqueeze(0).expand(batch, cons, num)\n",
    "        neg_head = self.neg_head.unsqueeze(0).expand(batch, cons, num)\n",
    "        \n",
    "        # batch x num\n",
    "        pos_head_max = torch.max(body_min * pos_head, dim=1).values.float()\n",
    "        neg_head_max = torch.max(body_min * neg_head, dim=1).values.float()\n",
    "        preds = torch.maximum(pos_head_max, torch.minimum(1 - neg_head_max, preds.squeeze()))\n",
    "        return preds\n",
    "        \n",
    "constraints_group = ConstraintsGroup([\n",
    "    Constraint('1 :- 0'),\n",
    "    Constraint('2 :- n3 4'),\n",
    "    Constraint('n5 :- 6 n7 8'),\n",
    "    Constraint('2 :- 9 n10'),\n",
    "    Constraint('n5 :- 11 n12 n13'),\n",
    "])\n",
    "\n",
    "cm = ConstraintsModule(constraints_group, 14)\n",
    "print(cm)\n",
    "preds = torch.rand((1000, 14))\n",
    "updated = cm(preds)\n",
    "assert constraints_group.coherent_with(updated.numpy()).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConstraintsLayer(\n",
      "  (module_list): ModuleList(\n",
      "    (0): ConstraintsModule()\n",
      "    (1): ConstraintsModule()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ConstraintsLayer(nn.Module):\n",
    "    def __init__(self, constraints_group, num_classes):\n",
    "        super(ConstraintsLayer, self).__init__()\n",
    "        strata = constraints_group.strata()\n",
    "        modules = [ConstraintsModule(stratum, num_classes) for stratum in strata]\n",
    "        self.module_list = nn.ModuleList(modules)\n",
    "        self.active = True\n",
    "        \n",
    "    def enable(self):\n",
    "        self.active = True\n",
    "        \n",
    "    def disable(self):\n",
    "        self.active = False\n",
    "        \n",
    "    def is_active(self):\n",
    "        return self.active\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.active:\n",
    "            for module in self.module_list:\n",
    "                x = module(x)\n",
    "        return x\n",
    "    \n",
    "constraints_layer = ConstraintsLayer(constraints_group, 14)\n",
    "print(constraints_layer)\n",
    "pred = torch.rand(1000, 14)\n",
    "updated = constraints_layer(pred)\n",
    "assert constraints_group.coherent_with(updated.numpy()).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConstrainedNetwork(\n",
      "  (base): NeuralNetwork(\n",
      "    (linear): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=4, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=4, out_features=2, bias=True)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (constraints): ConstraintsLayer(\n",
      "    (module_list): ModuleList(\n",
      "      (0): ConstraintsModule()\n",
      "      (1): ConstraintsModule()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ConstrainedNetwork(nn.Module):\n",
    "    def __init__(self, base, constraints, num_classes):\n",
    "        super(ConstrainedNetwork, self).__init__()\n",
    "        self.base = base\n",
    "        self.constraints = ConstraintsLayer(constraints, num_classes) \n",
    "        \n",
    "    def enable_constraints(self):\n",
    "        self.constraints.enable()\n",
    "        \n",
    "    def disable_constraints(self):\n",
    "        self.constraints.disable()\n",
    "        \n",
    "    def constraints_active(self):\n",
    "        return self.constraints.is_active()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        preds = self.base(x)\n",
    "        preds = self.constraints(preds)\n",
    "        return preds\n",
    "    \n",
    "model = ConstrainedNetwork(base, ConstraintsGroup([cons0]), 2)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "learning_rate = 1e-2\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, betas = (0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):   \n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    test_loss, correct_small, correct_big = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct_small += (torch.where(pred[:, 0] > 0.5, 1., 0.) == y[:, 0]).sum().item()\n",
    "            correct_big += (torch.where(pred[:, 1] > 0.5, 1., 0.) == y[:, 1]).sum().item()\n",
    "    test_loss /= size\n",
    "    correct_small /= size\n",
    "    correct_big /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct_small):>0.1f}%, {(100*correct_big):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing on the Rectangles dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.282106 [    0/10000]\n",
      "loss: 0.154154 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, 85.3%, Avg loss: 0.003057 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.281498 [    0/10000]\n",
      "loss: 0.154038 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, 85.3%, Avg loss: 0.003055 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.281355 [    0/10000]\n",
      "loss: 0.153919 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, 85.3%, Avg loss: 0.003054 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.281215 [    0/10000]\n",
      "loss: 0.153799 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, 85.3%, Avg loss: 0.003052 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.281071 [    0/10000]\n",
      "loss: 0.153721 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, 85.2%, Avg loss: 0.003050 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.280933 [    0/10000]\n",
      "loss: 0.153587 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, 85.2%, Avg loss: 0.003048 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.280750 [    0/10000]\n",
      "loss: 0.153443 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, 85.2%, Avg loss: 0.003047 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.280548 [    0/10000]\n",
      "loss: 0.153360 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, 85.1%, Avg loss: 0.003045 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.280391 [    0/10000]\n",
      "loss: 0.153237 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, 85.1%, Avg loss: 0.003043 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.280199 [    0/10000]\n",
      "loss: 0.153107 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, 85.1%, Avg loss: 0.003041 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.279992 [    0/10000]\n",
      "loss: 0.152967 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, 85.1%, Avg loss: 0.003039 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.279753 [    0/10000]\n",
      "loss: 0.152832 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, 85.1%, Avg loss: 0.003037 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.279517 [    0/10000]\n",
      "loss: 0.152697 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, 85.1%, Avg loss: 0.003035 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.279250 [    0/10000]\n",
      "loss: 0.152560 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, 85.0%, Avg loss: 0.003033 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.278994 [    0/10000]\n",
      "loss: 0.152423 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, 85.0%, Avg loss: 0.003031 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.278698 [    0/10000]\n",
      "loss: 0.152282 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, 85.0%, Avg loss: 0.003029 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.278366 [    0/10000]\n",
      "loss: 0.152278 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, 85.0%, Avg loss: 0.003026 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.278238 [    0/10000]\n",
      "loss: 0.152117 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, 85.0%, Avg loss: 0.003024 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.277861 [    0/10000]\n",
      "loss: 0.151962 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, 85.0%, Avg loss: 0.003021 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.277479 [    0/10000]\n",
      "loss: 0.151793 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, 85.0%, Avg loss: 0.003018 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.277045 [    0/10000]\n",
      "loss: 0.151595 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, 85.1%, Avg loss: 0.003016 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.276576 [    0/10000]\n",
      "loss: 0.151419 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, 85.1%, Avg loss: 0.003013 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.276065 [    0/10000]\n",
      "loss: 0.151244 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, 85.1%, Avg loss: 0.003010 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.275444 [    0/10000]\n",
      "loss: 0.151087 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, 85.1%, Avg loss: 0.003007 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.274877 [    0/10000]\n",
      "loss: 0.150854 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, 85.1%, Avg loss: 0.003003 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.274236 [    0/10000]\n",
      "loss: 0.150697 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, 85.0%, Avg loss: 0.003000 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.273624 [    0/10000]\n",
      "loss: 0.150459 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, 84.9%, Avg loss: 0.002996 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.272966 [    0/10000]\n",
      "loss: 0.150251 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, 85.0%, Avg loss: 0.002992 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.272194 [    0/10000]\n",
      "loss: 0.150033 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, 85.1%, Avg loss: 0.002988 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.271388 [    0/10000]\n",
      "loss: 0.149799 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, 85.1%, Avg loss: 0.002984 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.270370 [    0/10000]\n",
      "loss: 0.149473 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, 85.1%, Avg loss: 0.002979 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.269482 [    0/10000]\n",
      "loss: 0.149214 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, 85.0%, Avg loss: 0.002974 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.268549 [    0/10000]\n",
      "loss: 0.148946 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, 85.0%, Avg loss: 0.002968 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.267523 [    0/10000]\n",
      "loss: 0.148633 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, 85.0%, Avg loss: 0.002963 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.266499 [    0/10000]\n",
      "loss: 0.148360 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, 85.0%, Avg loss: 0.002957 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.265423 [    0/10000]\n",
      "loss: 0.148101 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, 85.0%, Avg loss: 0.002950 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.264290 [    0/10000]\n",
      "loss: 0.147778 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, 85.0%, Avg loss: 0.002944 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.263128 [    0/10000]\n",
      "loss: 0.147376 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, 85.0%, Avg loss: 0.002936 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.261926 [    0/10000]\n",
      "loss: 0.147012 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, 85.1%, Avg loss: 0.002928 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.260695 [    0/10000]\n",
      "loss: 0.146639 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, 85.1%, Avg loss: 0.002920 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.259432 [    0/10000]\n",
      "loss: 0.146252 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, 85.1%, Avg loss: 0.002911 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.258138 [    0/10000]\n",
      "loss: 0.145851 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.3%, 85.2%, Avg loss: 0.002902 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.256819 [    0/10000]\n",
      "loss: 0.145439 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.4%, 85.3%, Avg loss: 0.002891 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.255477 [    0/10000]\n",
      "loss: 0.145016 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.4%, 85.3%, Avg loss: 0.002881 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.254117 [    0/10000]\n",
      "loss: 0.144578 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.4%, 85.5%, Avg loss: 0.002869 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.252739 [    0/10000]\n",
      "loss: 0.144158 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.4%, 85.5%, Avg loss: 0.002857 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.251341 [    0/10000]\n",
      "loss: 0.143718 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.4%, 85.5%, Avg loss: 0.002845 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.249933 [    0/10000]\n",
      "loss: 0.143240 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.4%, 85.5%, Avg loss: 0.002832 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.248240 [    0/10000]\n",
      "loss: 0.142786 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.4%, 85.5%, Avg loss: 0.002818 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.246850 [    0/10000]\n",
      "loss: 0.142374 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.6%, 85.8%, Avg loss: 0.002804 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.245457 [    0/10000]\n",
      "loss: 0.141986 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.7%, 85.9%, Avg loss: 0.002789 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.244037 [    0/10000]\n",
      "loss: 0.141590 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.8%, 86.1%, Avg loss: 0.002774 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.242607 [    0/10000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.141203 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.8%, 86.1%, Avg loss: 0.002758 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.241179 [    0/10000]\n",
      "loss: 0.140823 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.8%, 86.0%, Avg loss: 0.002742 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.239770 [    0/10000]\n",
      "loss: 0.140452 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.7%, 86.0%, Avg loss: 0.002726 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.238374 [    0/10000]\n",
      "loss: 0.140109 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.7%, 86.2%, Avg loss: 0.002709 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.237162 [    0/10000]\n",
      "loss: 0.139675 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.7%, 86.3%, Avg loss: 0.002693 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.235679 [    0/10000]\n",
      "loss: 0.139291 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.7%, 86.6%, Avg loss: 0.002676 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.234325 [    0/10000]\n",
      "loss: 0.138965 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.8%, 87.5%, Avg loss: 0.002659 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.232963 [    0/10000]\n",
      "loss: 0.138655 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.9%, 88.1%, Avg loss: 0.002642 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.231587 [    0/10000]\n",
      "loss: 0.138098 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.9%, 88.7%, Avg loss: 0.002625 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.230004 [    0/10000]\n",
      "loss: 0.137822 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.9%, 89.1%, Avg loss: 0.002608 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.228731 [    0/10000]\n",
      "loss: 0.137534 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 95.9%, 89.8%, Avg loss: 0.002592 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.227478 [    0/10000]\n",
      "loss: 0.137235 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.0%, 90.4%, Avg loss: 0.002575 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.226241 [    0/10000]\n",
      "loss: 0.136923 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.0%, 90.6%, Avg loss: 0.002559 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.225019 [    0/10000]\n",
      "loss: 0.136602 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.0%, 90.9%, Avg loss: 0.002543 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.223807 [    0/10000]\n",
      "loss: 0.136273 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.0%, 91.6%, Avg loss: 0.002527 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.222606 [    0/10000]\n",
      "loss: 0.135931 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.0%, 92.1%, Avg loss: 0.002511 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.221427 [    0/10000]\n",
      "loss: 0.135575 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.1%, 92.1%, Avg loss: 0.002495 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.220316 [    0/10000]\n",
      "loss: 0.135199 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.3%, 92.6%, Avg loss: 0.002480 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.219160 [    0/10000]\n",
      "loss: 0.134750 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.3%, 92.8%, Avg loss: 0.002465 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.218021 [    0/10000]\n",
      "loss: 0.134313 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.5%, 92.9%, Avg loss: 0.002450 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.216900 [    0/10000]\n",
      "loss: 0.133853 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.5%, 92.9%, Avg loss: 0.002435 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.215784 [    0/10000]\n",
      "loss: 0.133371 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.5%, 93.1%, Avg loss: 0.002421 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.214577 [    0/10000]\n",
      "loss: 0.132892 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.5%, 93.2%, Avg loss: 0.002406 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.213485 [    0/10000]\n",
      "loss: 0.132369 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.5%, 93.3%, Avg loss: 0.002392 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.212375 [    0/10000]\n",
      "loss: 0.131832 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.5%, 93.3%, Avg loss: 0.002378 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.211299 [    0/10000]\n",
      "loss: 0.131264 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.5%, 93.3%, Avg loss: 0.002365 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.210230 [    0/10000]\n",
      "loss: 0.130672 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.4%, 93.1%, Avg loss: 0.002351 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.209207 [    0/10000]\n",
      "loss: 0.130066 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.5%, 93.2%, Avg loss: 0.002338 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.208166 [    0/10000]\n",
      "loss: 0.129465 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.4%, 93.3%, Avg loss: 0.002325 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.207151 [    0/10000]\n",
      "loss: 0.129085 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.4%, 93.4%, Avg loss: 0.002312 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.206124 [    0/10000]\n",
      "loss: 0.128393 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.4%, 93.4%, Avg loss: 0.002300 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.205117 [    0/10000]\n",
      "loss: 0.127711 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.3%, 93.4%, Avg loss: 0.002287 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.204124 [    0/10000]\n",
      "loss: 0.127026 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.3%, 93.5%, Avg loss: 0.002275 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.203155 [    0/10000]\n",
      "loss: 0.126299 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.4%, 93.7%, Avg loss: 0.002263 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.202141 [    0/10000]\n",
      "loss: 0.125519 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.4%, 93.7%, Avg loss: 0.002252 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.201233 [    0/10000]\n",
      "loss: 0.124767 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.4%, 93.7%, Avg loss: 0.002240 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.200243 [    0/10000]\n",
      "loss: 0.124030 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.4%, 93.7%, Avg loss: 0.002229 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.199307 [    0/10000]\n",
      "loss: 0.123394 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.4%, 93.8%, Avg loss: 0.002218 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.198407 [    0/10000]\n",
      "loss: 0.122662 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.4%, 93.8%, Avg loss: 0.002208 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.197516 [    0/10000]\n",
      "loss: 0.121916 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.4%, 93.8%, Avg loss: 0.002197 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.196651 [    0/10000]\n",
      "loss: 0.121154 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.4%, 93.8%, Avg loss: 0.002187 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.195794 [    0/10000]\n",
      "loss: 0.120533 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.4%, 93.8%, Avg loss: 0.002177 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.194968 [    0/10000]\n",
      "loss: 0.119805 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.4%, 93.9%, Avg loss: 0.002167 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.194115 [    0/10000]\n",
      "loss: 0.119069 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.4%, 93.9%, Avg loss: 0.002158 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.193309 [    0/10000]\n",
      "loss: 0.118347 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.5%, 93.9%, Avg loss: 0.002148 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.192461 [    0/10000]\n",
      "loss: 0.117765 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.5%, 94.0%, Avg loss: 0.002139 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.191654 [    0/10000]\n",
      "loss: 0.117047 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.5%, 94.0%, Avg loss: 0.002130 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.190903 [    0/10000]\n",
      "loss: 0.116326 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.5%, 94.0%, Avg loss: 0.002121 \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 0.190168 [    0/10000]\n",
      "loss: 0.115614 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.6%, 94.2%, Avg loss: 0.002113 \n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 0.189441 [    0/10000]\n",
      "loss: 0.114896 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.7%, 94.2%, Avg loss: 0.002105 \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 0.188732 [    0/10000]\n",
      "loss: 0.114168 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.8%, 94.2%, Avg loss: 0.002096 \n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 0.187988 [    0/10000]\n",
      "loss: 0.113456 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.8%, 94.2%, Avg loss: 0.002088 \n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 0.187301 [    0/10000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.112739 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.8%, 94.2%, Avg loss: 0.002080 \n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 0.186643 [    0/10000]\n",
      "loss: 0.112190 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.8%, 94.2%, Avg loss: 0.002073 \n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 0.186027 [    0/10000]\n",
      "loss: 0.111536 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.8%, 94.2%, Avg loss: 0.002065 \n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 0.185403 [    0/10000]\n",
      "loss: 0.110895 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.8%, 94.1%, Avg loss: 0.002058 \n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 0.184792 [    0/10000]\n",
      "loss: 0.110266 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, 94.3%, Avg loss: 0.002051 \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 0.184193 [    0/10000]\n",
      "loss: 0.109649 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, 94.4%, Avg loss: 0.002044 \n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 0.183832 [    0/10000]\n",
      "loss: 0.109069 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, 94.4%, Avg loss: 0.002039 \n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 0.183224 [    0/10000]\n",
      "loss: 0.108465 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, 94.4%, Avg loss: 0.002032 \n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 0.182676 [    0/10000]\n",
      "loss: 0.107865 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, 94.4%, Avg loss: 0.002025 \n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 0.182142 [    0/10000]\n",
      "loss: 0.107325 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, 94.4%, Avg loss: 0.002019 \n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "loss: 0.181633 [    0/10000]\n",
      "loss: 0.106828 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, 94.5%, Avg loss: 0.002013 \n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "loss: 0.181125 [    0/10000]\n",
      "loss: 0.106142 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, 94.5%, Avg loss: 0.002006 \n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "loss: 0.180652 [    0/10000]\n",
      "loss: 0.105811 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, 94.4%, Avg loss: 0.002000 \n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "loss: 0.180166 [    0/10000]\n",
      "loss: 0.105308 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, 94.4%, Avg loss: 0.001994 \n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "loss: 0.179703 [    0/10000]\n",
      "loss: 0.104819 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, 94.4%, Avg loss: 0.001988 \n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "loss: 0.179243 [    0/10000]\n",
      "loss: 0.104342 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, 94.4%, Avg loss: 0.001982 \n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "loss: 0.178767 [    0/10000]\n",
      "loss: 0.103939 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, 94.4%, Avg loss: 0.001976 \n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "loss: 0.178319 [    0/10000]\n",
      "loss: 0.103506 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, 94.4%, Avg loss: 0.001971 \n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "loss: 0.177998 [    0/10000]\n",
      "loss: 0.103052 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.5%, Avg loss: 0.001965 \n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "loss: 0.177557 [    0/10000]\n",
      "loss: 0.102619 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.5%, Avg loss: 0.001960 \n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "loss: 0.177132 [    0/10000]\n",
      "loss: 0.102195 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.5%, Avg loss: 0.001955 \n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "loss: 0.176723 [    0/10000]\n",
      "loss: 0.101786 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.6%, Avg loss: 0.001949 \n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "loss: 0.176263 [    0/10000]\n",
      "loss: 0.101401 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.6%, Avg loss: 0.001944 \n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "loss: 0.175872 [    0/10000]\n",
      "loss: 0.101036 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.6%, Avg loss: 0.001939 \n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "loss: 0.175517 [    0/10000]\n",
      "loss: 0.100669 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.6%, Avg loss: 0.001934 \n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "loss: 0.175153 [    0/10000]\n",
      "loss: 0.100312 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.7%, Avg loss: 0.001929 \n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "loss: 0.174796 [    0/10000]\n",
      "loss: 0.099963 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.7%, Avg loss: 0.001924 \n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "loss: 0.174447 [    0/10000]\n",
      "loss: 0.099623 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.7%, Avg loss: 0.001919 \n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "loss: 0.174106 [    0/10000]\n",
      "loss: 0.099291 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.7%, Avg loss: 0.001914 \n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "loss: 0.173748 [    0/10000]\n",
      "loss: 0.098953 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.7%, Avg loss: 0.001910 \n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "loss: 0.173424 [    0/10000]\n",
      "loss: 0.098634 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.7%, Avg loss: 0.001905 \n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "loss: 0.173107 [    0/10000]\n",
      "loss: 0.098640 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.7%, Avg loss: 0.001901 \n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "loss: 0.172794 [    0/10000]\n",
      "loss: 0.098237 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.7%, Avg loss: 0.001896 \n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "loss: 0.172474 [    0/10000]\n",
      "loss: 0.097934 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.7%, Avg loss: 0.001892 \n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "loss: 0.172175 [    0/10000]\n",
      "loss: 0.097646 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.7%, Avg loss: 0.001888 \n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "loss: 0.171884 [    0/10000]\n",
      "loss: 0.097367 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.7%, Avg loss: 0.001883 \n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "loss: 0.171960 [    0/10000]\n",
      "loss: 0.097071 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.7%, Avg loss: 0.001879 \n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "loss: 0.171675 [    0/10000]\n",
      "loss: 0.096776 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.7%, Avg loss: 0.001875 \n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "loss: 0.171417 [    0/10000]\n",
      "loss: 0.096526 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.7%, Avg loss: 0.001870 \n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "loss: 0.171153 [    0/10000]\n",
      "loss: 0.096282 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.7%, Avg loss: 0.001866 \n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "loss: 0.170894 [    0/10000]\n",
      "loss: 0.096044 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.7%, Avg loss: 0.001862 \n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "loss: 0.170641 [    0/10000]\n",
      "loss: 0.095812 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.7%, Avg loss: 0.001858 \n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "loss: 0.170393 [    0/10000]\n",
      "loss: 0.095585 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.7%, Avg loss: 0.001854 \n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "loss: 0.170057 [    0/10000]\n",
      "loss: 0.095351 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.8%, Avg loss: 0.001850 \n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "loss: 0.169813 [    0/10000]\n",
      "loss: 0.095130 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, 94.7%, Avg loss: 0.001846 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "loss: 0.169573 [    0/10000]\n",
      "loss: 0.094918 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, 94.7%, Avg loss: 0.001843 \n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "loss: 0.169337 [    0/10000]\n",
      "loss: 0.094710 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, 94.7%, Avg loss: 0.001839 \n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "loss: 0.169106 [    0/10000]\n",
      "loss: 0.094507 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, 94.7%, Avg loss: 0.001835 \n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "loss: 0.168880 [    0/10000]\n",
      "loss: 0.094309 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, 94.7%, Avg loss: 0.001832 \n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "loss: 0.168658 [    0/10000]\n",
      "loss: 0.094115 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, 94.7%, Avg loss: 0.001828 \n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "loss: 0.168442 [    0/10000]\n",
      "loss: 0.093925 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, 94.7%, Avg loss: 0.001824 \n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "loss: 0.168221 [    0/10000]\n",
      "loss: 0.093731 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, 94.7%, Avg loss: 0.001821 \n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "loss: 0.168039 [    0/10000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.093527 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, 94.6%, Avg loss: 0.001818 \n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "loss: 0.167793 [    0/10000]\n",
      "loss: 0.093320 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, 94.7%, Avg loss: 0.001814 \n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "loss: 0.167541 [    0/10000]\n",
      "loss: 0.093340 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.7%, Avg loss: 0.001811 \n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "loss: 0.167418 [    0/10000]\n",
      "loss: 0.093197 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.8%, Avg loss: 0.001808 \n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "loss: 0.167214 [    0/10000]\n",
      "loss: 0.093008 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.8%, Avg loss: 0.001804 \n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "loss: 0.166989 [    0/10000]\n",
      "loss: 0.092842 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.8%, Avg loss: 0.001801 \n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "loss: 0.166781 [    0/10000]\n",
      "loss: 0.092680 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.8%, Avg loss: 0.001798 \n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "loss: 0.166574 [    0/10000]\n",
      "loss: 0.092514 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.8%, Avg loss: 0.001795 \n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "loss: 0.166369 [    0/10000]\n",
      "loss: 0.092357 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.8%, Avg loss: 0.001792 \n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "loss: 0.166168 [    0/10000]\n",
      "loss: 0.092173 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.8%, Avg loss: 0.001789 \n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "loss: 0.165928 [    0/10000]\n",
      "loss: 0.092013 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.8%, Avg loss: 0.001786 \n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "loss: 0.165725 [    0/10000]\n",
      "loss: 0.091858 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.8%, Avg loss: 0.001783 \n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "loss: 0.165528 [    0/10000]\n",
      "loss: 0.091709 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.8%, Avg loss: 0.001781 \n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "loss: 0.164646 [    0/10000]\n",
      "loss: 0.091518 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.8%, Avg loss: 0.001778 \n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "loss: 0.164453 [    0/10000]\n",
      "loss: 0.091244 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.8%, Avg loss: 0.001775 \n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "loss: 0.164231 [    0/10000]\n",
      "loss: 0.091134 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.8%, Avg loss: 0.001772 \n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "loss: 0.164039 [    0/10000]\n",
      "loss: 0.090988 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.8%, Avg loss: 0.001769 \n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "loss: 0.163852 [    0/10000]\n",
      "loss: 0.090851 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, 94.7%, Avg loss: 0.001766 \n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "loss: 0.163669 [    0/10000]\n",
      "loss: 0.090718 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, 94.7%, Avg loss: 0.001764 \n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "loss: 0.163492 [    0/10000]\n",
      "loss: 0.090708 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, 94.7%, Avg loss: 0.001761 \n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "loss: 0.163301 [    0/10000]\n",
      "loss: 0.090580 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, 94.7%, Avg loss: 0.001758 \n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "loss: 0.163129 [    0/10000]\n",
      "loss: 0.090468 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, 94.7%, Avg loss: 0.001755 \n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "loss: 0.162958 [    0/10000]\n",
      "loss: 0.090359 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, 94.7%, Avg loss: 0.001753 \n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "loss: 0.162804 [    0/10000]\n",
      "loss: 0.090233 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, 94.7%, Avg loss: 0.001750 \n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "loss: 0.162641 [    0/10000]\n",
      "loss: 0.090129 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, 94.7%, Avg loss: 0.001748 \n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "loss: 0.162433 [    0/10000]\n",
      "loss: 0.090007 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, 94.7%, Avg loss: 0.001745 \n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "loss: 0.162275 [    0/10000]\n",
      "loss: 0.089809 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.8%, Avg loss: 0.001743 \n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "loss: 0.162114 [    0/10000]\n",
      "loss: 0.089691 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.8%, Avg loss: 0.001740 \n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "loss: 0.161960 [    0/10000]\n",
      "loss: 0.089577 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001738 \n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "loss: 0.161811 [    0/10000]\n",
      "loss: 0.089465 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001735 \n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "loss: 0.161664 [    0/10000]\n",
      "loss: 0.089355 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001733 \n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "loss: 0.161520 [    0/10000]\n",
      "loss: 0.089248 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001729 \n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "loss: 0.161557 [    0/10000]\n",
      "loss: 0.089148 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001727 \n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "loss: 0.161422 [    0/10000]\n",
      "loss: 0.089047 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001724 \n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "loss: 0.161285 [    0/10000]\n",
      "loss: 0.088946 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001722 \n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "loss: 0.161151 [    0/10000]\n",
      "loss: 0.088699 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001720 \n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "loss: 0.161015 [    0/10000]\n",
      "loss: 0.088606 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001718 \n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "loss: 0.160805 [    0/10000]\n",
      "loss: 0.088514 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001716 \n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "loss: 0.160666 [    0/10000]\n",
      "loss: 0.088412 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001714 \n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "loss: 0.160539 [    0/10000]\n",
      "loss: 0.088312 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001711 \n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "loss: 0.160413 [    0/10000]\n",
      "loss: 0.088212 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001709 \n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "loss: 0.160294 [    0/10000]\n",
      "loss: 0.088114 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001707 \n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "loss: 0.160172 [    0/10000]\n",
      "loss: 0.088019 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001705 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "loss: 0.160053 [    0/10000]\n",
      "loss: 0.087955 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001703 \n",
      "\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "loss: 0.159938 [    0/10000]\n",
      "loss: 0.087861 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001701 \n",
      "\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "loss: 0.159825 [    0/10000]\n",
      "loss: 0.087767 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001699 \n",
      "\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "loss: 0.159713 [    0/10000]\n",
      "loss: 0.087703 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001697 \n",
      "\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "loss: 0.159595 [    0/10000]\n",
      "loss: 0.087578 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001695 \n",
      "\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "loss: 0.159480 [    0/10000]\n",
      "loss: 0.087483 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001693 \n",
      "\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "loss: 0.159365 [    0/10000]\n",
      "loss: 0.087378 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001691 \n",
      "\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "loss: 0.159251 [    0/10000]\n",
      "loss: 0.087292 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001689 \n",
      "\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "loss: 0.159139 [    0/10000]\n",
      "loss: 0.087208 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001687 \n",
      "\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "loss: 0.159036 [    0/10000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.087125 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001685 \n",
      "\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "loss: 0.158931 [    0/10000]\n",
      "loss: 0.087044 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001684 \n",
      "\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "loss: 0.158836 [    0/10000]\n",
      "loss: 0.086964 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001682 \n",
      "\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "loss: 0.158769 [    0/10000]\n",
      "loss: 0.086878 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001680 \n",
      "\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "loss: 0.158664 [    0/10000]\n",
      "loss: 0.086796 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001678 \n",
      "\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "loss: 0.158513 [    0/10000]\n",
      "loss: 0.086705 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001676 \n",
      "\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "loss: 0.158415 [    0/10000]\n",
      "loss: 0.086627 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001675 \n",
      "\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "loss: 0.158318 [    0/10000]\n",
      "loss: 0.086551 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001673 \n",
      "\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "loss: 0.158223 [    0/10000]\n",
      "loss: 0.086476 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001671 \n",
      "\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "loss: 0.158125 [    0/10000]\n",
      "loss: 0.086401 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001669 \n",
      "\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "loss: 0.158030 [    0/10000]\n",
      "loss: 0.086327 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001668 \n",
      "\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "loss: 0.157934 [    0/10000]\n",
      "loss: 0.086256 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001665 \n",
      "\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "loss: 0.157798 [    0/10000]\n",
      "loss: 0.086187 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001664 \n",
      "\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "loss: 0.157710 [    0/10000]\n",
      "loss: 0.086039 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001662 \n",
      "\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "loss: 0.157632 [    0/10000]\n",
      "loss: 0.085968 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001661 \n",
      "\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "loss: 0.157543 [    0/10000]\n",
      "loss: 0.085899 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001659 \n",
      "\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "loss: 0.157453 [    0/10000]\n",
      "loss: 0.085832 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001657 \n",
      "\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "loss: 0.157368 [    0/10000]\n",
      "loss: 0.085765 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001656 \n",
      "\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "loss: 0.157285 [    0/10000]\n",
      "loss: 0.085697 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001655 \n",
      "\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "loss: 0.157807 [    0/10000]\n",
      "loss: 0.085603 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001653 \n",
      "\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "loss: 0.157720 [    0/10000]\n",
      "loss: 0.085536 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001651 \n",
      "\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "loss: 0.157636 [    0/10000]\n",
      "loss: 0.085501 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001650 \n",
      "\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "loss: 0.157536 [    0/10000]\n",
      "loss: 0.085437 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001648 \n",
      "\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "loss: 0.157456 [    0/10000]\n",
      "loss: 0.085375 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001647 \n",
      "\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "loss: 0.157379 [    0/10000]\n",
      "loss: 0.085314 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001645 \n",
      "\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "loss: 0.157304 [    0/10000]\n",
      "loss: 0.085254 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001644 \n",
      "\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "loss: 0.157230 [    0/10000]\n",
      "loss: 0.085194 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001642 \n",
      "\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "loss: 0.157123 [    0/10000]\n",
      "loss: 0.085134 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001641 \n",
      "\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "loss: 0.157055 [    0/10000]\n",
      "loss: 0.085074 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001640 \n",
      "\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "loss: 0.156952 [    0/10000]\n",
      "loss: 0.085018 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001638 \n",
      "\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "loss: 0.156879 [    0/10000]\n",
      "loss: 0.084947 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001637 \n",
      "\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "loss: 0.156818 [    0/10000]\n",
      "loss: 0.084878 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001636 \n",
      "\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "loss: 0.156749 [    0/10000]\n",
      "loss: 0.084821 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001634 \n",
      "\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "loss: 0.156684 [    0/10000]\n",
      "loss: 0.084765 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001633 \n",
      "\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "loss: 0.156621 [    0/10000]\n",
      "loss: 0.084709 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001631 \n",
      "\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "loss: 0.156558 [    0/10000]\n",
      "loss: 0.084654 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001630 \n",
      "\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "loss: 0.156496 [    0/10000]\n",
      "loss: 0.084599 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001629 \n",
      "\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "loss: 0.156602 [    0/10000]\n",
      "loss: 0.084520 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001627 \n",
      "\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "loss: 0.156528 [    0/10000]\n",
      "loss: 0.084451 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001626 \n",
      "\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "loss: 0.156466 [    0/10000]\n",
      "loss: 0.084389 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001625 \n",
      "\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "loss: 0.156404 [    0/10000]\n",
      "loss: 0.084331 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001623 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "loss: 0.156343 [    0/10000]\n",
      "loss: 0.084276 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001622 \n",
      "\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "loss: 0.156283 [    0/10000]\n",
      "loss: 0.084223 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001621 \n",
      "\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "loss: 0.156224 [    0/10000]\n",
      "loss: 0.084172 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001619 \n",
      "\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "loss: 0.156110 [    0/10000]\n",
      "loss: 0.084119 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001618 \n",
      "\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "loss: 0.156052 [    0/10000]\n",
      "loss: 0.084069 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001617 \n",
      "\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "loss: 0.155994 [    0/10000]\n",
      "loss: 0.084017 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001615 \n",
      "\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "loss: 0.155935 [    0/10000]\n",
      "loss: 0.083961 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001614 \n",
      "\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "loss: 0.155817 [    0/10000]\n",
      "loss: 0.083909 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001613 \n",
      "\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "loss: 0.155758 [    0/10000]\n",
      "loss: 0.083861 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001612 \n",
      "\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "loss: 0.155702 [    0/10000]\n",
      "loss: 0.083814 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001610 \n",
      "\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "loss: 0.155648 [    0/10000]\n",
      "loss: 0.083754 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001609 \n",
      "\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "loss: 0.155602 [    0/10000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.083751 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001608 \n",
      "\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "loss: 0.155538 [    0/10000]\n",
      "loss: 0.083689 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001607 \n",
      "\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "loss: 0.155476 [    0/10000]\n",
      "loss: 0.083643 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001606 \n",
      "\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "loss: 0.155423 [    0/10000]\n",
      "loss: 0.083598 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001604 \n",
      "\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "loss: 0.155373 [    0/10000]\n",
      "loss: 0.083553 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001603 \n",
      "\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "loss: 0.155323 [    0/10000]\n",
      "loss: 0.083508 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001602 \n",
      "\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "loss: 0.155273 [    0/10000]\n",
      "loss: 0.083464 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001601 \n",
      "\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "loss: 0.155225 [    0/10000]\n",
      "loss: 0.083416 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001600 \n",
      "\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "loss: 0.155171 [    0/10000]\n",
      "loss: 0.083368 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001599 \n",
      "\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "loss: 0.155121 [    0/10000]\n",
      "loss: 0.083324 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001598 \n",
      "\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "loss: 0.155072 [    0/10000]\n",
      "loss: 0.083280 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001596 \n",
      "\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "loss: 0.155023 [    0/10000]\n",
      "loss: 0.083235 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001595 \n",
      "\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "loss: 0.154975 [    0/10000]\n",
      "loss: 0.083180 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001594 \n",
      "\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "loss: 0.154925 [    0/10000]\n",
      "loss: 0.083138 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001593 \n",
      "\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "loss: 0.154888 [    0/10000]\n",
      "loss: 0.083098 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001592 \n",
      "\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "loss: 0.154845 [    0/10000]\n",
      "loss: 0.083057 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001590 \n",
      "\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "loss: 0.155008 [    0/10000]\n",
      "loss: 0.083021 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001589 \n",
      "\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "loss: 0.154966 [    0/10000]\n",
      "loss: 0.082981 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001588 \n",
      "\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "loss: 0.154926 [    0/10000]\n",
      "loss: 0.082941 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001587 \n",
      "\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "loss: 0.154850 [    0/10000]\n",
      "loss: 0.082900 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001586 \n",
      "\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "loss: 0.154809 [    0/10000]\n",
      "loss: 0.082908 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001585 \n",
      "\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "loss: 0.154755 [    0/10000]\n",
      "loss: 0.082866 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001584 \n",
      "\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "loss: 0.154716 [    0/10000]\n",
      "loss: 0.082826 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001583 \n",
      "\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "loss: 0.154674 [    0/10000]\n",
      "loss: 0.082788 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 95.0%, Avg loss: 0.001582 \n",
      "\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "loss: 0.154634 [    0/10000]\n",
      "loss: 0.082750 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 95.0%, Avg loss: 0.001581 \n",
      "\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "loss: 0.154594 [    0/10000]\n",
      "loss: 0.082712 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001580 \n",
      "\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "loss: 0.154556 [    0/10000]\n",
      "loss: 0.082675 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001579 \n",
      "\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "loss: 0.154518 [    0/10000]\n",
      "loss: 0.082639 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001578 \n",
      "\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "loss: 0.154480 [    0/10000]\n",
      "loss: 0.082603 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001577 \n",
      "\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "loss: 0.154442 [    0/10000]\n",
      "loss: 0.082566 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001576 \n",
      "\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "loss: 0.154403 [    0/10000]\n",
      "loss: 0.082531 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001575 \n",
      "\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "loss: 0.154366 [    0/10000]\n",
      "loss: 0.082494 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001574 \n",
      "\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "loss: 0.154326 [    0/10000]\n",
      "loss: 0.082459 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001573 \n",
      "\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "loss: 0.154290 [    0/10000]\n",
      "loss: 0.082425 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001572 \n",
      "\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "loss: 0.154254 [    0/10000]\n",
      "loss: 0.082390 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001571 \n",
      "\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "loss: 0.154217 [    0/10000]\n",
      "loss: 0.082355 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001570 \n",
      "\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "loss: 0.154168 [    0/10000]\n",
      "loss: 0.082321 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001569 \n",
      "\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "loss: 0.154131 [    0/10000]\n",
      "loss: 0.082288 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001569 \n",
      "\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "loss: 0.154096 [    0/10000]\n",
      "loss: 0.082299 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001568 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "loss: 0.154043 [    0/10000]\n",
      "loss: 0.082263 [ 6400/10000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, 94.9%, Avg loss: 0.001567 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 300\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xs = np.arange(0., 1., 0.01, dtype = \"float32\")\n",
    "ys = np.arange(0., 1., 0.01, dtype = \"float32\")\n",
    "grid = np.array([(x, y) for x in xs for y in ys])\n",
    "grid = torch.from_numpy(grid)\n",
    "\n",
    "model.disable_constraints()\n",
    "probs = model(grid)\n",
    "model.enable_constraints()\n",
    "\n",
    "small = probs[:, 0].reshape(len(xs), len(ys)).detach().numpy()\n",
    "large = probs[:, 1].reshape(len(xs), len(ys)).detach().numpy()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(small, cmap='hot', interpolation='nearest', extent=(0., 1., 0., 1.), origin='lower')\n",
    "draw_rectangles(ax, rect0, rect1)\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(large, cmap='hot', interpolation='nearest', extent=(0., 1., 0., 1.), origin='lower')\n",
    "draw_rectangles(ax, rect0, rect1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9998578\n"
     ]
    }
   ],
   "source": [
    "print(small.max())\n",
    "print(large.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
